{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c3b1c46-9f5c-41c1-9101-85db8709ec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7a0db5-7f04-4845-8b11-684fe6e9f7f2",
   "metadata": {},
   "source": [
    "# Video segmentation with SAM 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ba7875-35e5-478b-b8ba-4b48e121dec7",
   "metadata": {},
   "source": [
    "This notebook shows how to use SAM 2 for interactive segmentation in videos. It will cover the following:\n",
    "\n",
    "- adding clicks (or box) on a frame to get and refine _masklets_ (spatio-temporal masks)\n",
    "- propagating clicks (or box) to get _masklets_ throughout the video\n",
    "- segmenting and tracking multiple objects at the same time\n",
    "\n",
    "We use the terms _segment_ or _mask_ to refer to the model prediction for an object on a single frame, and _masklet_ to refer to the spatio-temporal masks across the entire video. \n",
    "此笔记本介绍如何使用 SAM 2 在视频中进行交互式分段。它将涵盖以下内容：\n",
    "\n",
    "在帧上添加单击（或框）以获取和优化 masklets（时空掩码）\n",
    "在整个视频中传播单击（或框）以获取蒙版\n",
    "同时分割和跟踪多个对象\n",
    "我们使用术语 segment 或 mask 来指代对单个帧上对象的模型预测，使用 masklet 来指代整个视频中的时空掩码。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a887b90f-6576-4ef8-964e-76d3a156ccb6",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/facebookresearch/sam2/blob/main/notebooks/video_predictor_example.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26616201-06df-435b-98fd-ad17c373bb4a",
   "metadata": {},
   "source": [
    "## Environment Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8491a127-4c01-48f5-9dc5-f148a9417fdf",
   "metadata": {},
   "source": [
    "If running locally using jupyter, first install `sam2` in your environment using the [installation instructions](https://github.com/facebookresearch/sam2#installation) in the repository.\n",
    "\n",
    "If running from Google Colab, set `using_colab=True` below and run the cell. In Colab, be sure to select 'GPU' under 'Edit'->'Notebook Settings'->'Hardware accelerator'. Note that it's recommended to use **A100 or L4 GPUs when running in Colab** (T4 GPUs might also work, but could be slow and might run out of memory in some cases).\n",
    "\n",
    "如果使用 jupyter 在本地运行，请首先使用存储库中的 [安装说明]（https://github.com/facebookresearch/sam2#installation） 在您的环境中安装“sam2”。\n",
    "\n",
    "如果从 Google Colab 运行，请在下方设置 'using_colab=True' 并运行单元格。在 Colab 中，请务必在“编辑”->“笔记本设置”->“硬件加速器”下选择“GPU”。请注意，在 Colab 中运行时，建议使用 **A100 或 L4 GPU**（T4 GPU 也可能有效，但可能会很慢，并且在某些情况下可能会耗尽内存）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f74c53be-aab1-46b9-8c0b-068b52ef5948",
   "metadata": {},
   "outputs": [],
   "source": [
    "using_colab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d824a4b2-71f3-4da3-bfc7-3249625e6730",
   "metadata": {},
   "outputs": [],
   "source": [
    "if using_colab:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(\"PyTorch version:\", torch.__version__)\n",
    "    print(\"Torchvision version:\", torchvision.__version__)\n",
    "    print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install opencv-python matplotlib\n",
    "    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/sam2.git'\n",
    "\n",
    "    !mkdir -p videos\n",
    "    !wget -P videos https://dl.fbaipublicfiles.com/segment_anything_2/assets/bedroom.zip\n",
    "    !unzip -d videos videos/bedroom.zip\n",
    "\n",
    "    !mkdir -p ../checkpoints/\n",
    "    !wget -P ../checkpoints/ https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e6aa9d-487f-4207-b657-8cff0902343e",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5318a85-5bf7-4880-b2b3-15e4db24d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# if using Apple MPS, fall back to CPU for unsupported ops\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08ba49d8-8c22-4eba-a2ab-46eee839287f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# select the device for computation\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    # use bfloat16 for the entire notebook\n",
    "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "elif device.type == \"mps\":\n",
    "    print(\n",
    "        \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
    "        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
    "        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8e0779-751f-4224-9b04-ed0f0b406500",
   "metadata": {},
   "source": [
    "### Loading the SAM 2 video predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5f3245e-b4d6-418b-a42a-a67e0b3b5aec",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../checkpoints/sam2.1_hiera_large.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m sam2_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../checkpoints/sam2.1_hiera_large.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m model_cfg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigs/sam2.1/sam2.1_hiera_l.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 6\u001b[0m predictor \u001b[38;5;241m=\u001b[39m build_sam2_video_predictor(model_cfg, sam2_checkpoint, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32mF:\\SAM2\\SAM2\\sam2\\sam2\\build_sam.py:137\u001b[0m, in \u001b[0;36mbuild_sam2_video_predictor\u001b[1;34m(config_file, ckpt_path, device, mode, hydra_overrides_extra, apply_postprocessing, vos_optimized, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m OmegaConf\u001b[38;5;241m.\u001b[39mresolve(cfg)\n\u001b[0;32m    136\u001b[0m model \u001b[38;5;241m=\u001b[39m instantiate(cfg\u001b[38;5;241m.\u001b[39mmodel, _recursive_\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 137\u001b[0m _load_checkpoint(model, ckpt_path)\n\u001b[0;32m    138\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mF:\\SAM2\\SAM2\\sam2\\sam2\\build_sam.py:166\u001b[0m, in \u001b[0;36m_load_checkpoint\u001b[1;34m(model, ckpt_path)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_checkpoint\u001b[39m(model, ckpt_path):\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ckpt_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 166\u001b[0m         sd \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(ckpt_path, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    167\u001b[0m         missing_keys, unexpected_keys \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mload_state_dict(sd)\n\u001b[0;32m    168\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m missing_keys:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\serialization.py:1425\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1423\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1425\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m   1426\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1427\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1428\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1429\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1430\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\serialization.py:751\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    750\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 751\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    753\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\serialization.py:732\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 732\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mopen\u001b[39m(name, mode))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../checkpoints/sam2.1_hiera_large.pt'"
     ]
    }
   ],
   "source": [
    "from sam2.build_sam import build_sam2_video_predictor\n",
    "\n",
    "sam2_checkpoint = \"../checkpoints/sam2.1_hiera_large.pt\"\n",
    "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "\n",
    "predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5320fe-06d7-45b8-b888-ae00799d07fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, obj_id=None, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        cmap = plt.get_cmap(\"tab10\")\n",
    "        cmap_idx = 0 if obj_id is None else obj_id\n",
    "        color = np.array([*cmap(cmap_idx)[:3], 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=200):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22aa751-b7cd-451e-9ded-fb98bf4bdfad",
   "metadata": {},
   "source": [
    "#### Select an example video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4c6af6-e18d-4939-beaf-2bc00f94a724",
   "metadata": {},
   "source": [
    "We assume that the video is stored as a list of JPEG frames with filenames like `<frame_index>.jpg`.\n",
    "\n",
    "For your custom videos, you can extract their JPEG frames using ffmpeg (https://ffmpeg.org/) as follows:\n",
    "```\n",
    "ffmpeg -i <your_video>.mp4 -q:v 2 -start_number 0 <output_dir>/'%05d.jpg'\n",
    "```\n",
    "where `-q:v` generates high-quality JPEG frames and `-start_number 0` asks ffmpeg to start the JPEG file from `00000.jpg`.\n",
    "\n",
    "我们假设视频存储为文件名类似于 '.jpg' 的 JPEG 帧列表<frame_index>。\n",
    "\n",
    "对于您的自定义视频，您可以使用 ffmpeg （https://ffmpeg.org/） 提取其 JPEG 帧，如下所示：\n",
    "```\n",
    "ffmpeg -i <your_video>.mp4 -q：v 2 -start_number 0 <output_dir>/'d.jpg'\n",
    "```\n",
    "其中 '-q：v' 生成高质量的 JPEG 帧，'-start_number 0' 要求 ffmpeg 从 '00000.jpg' 开始 JPEG 文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94c87ca-fd1a-4011-9609-e8be1cbe3230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `video_dir` a directory of JPEG frames with filenames like `<frame_index>.jpg`\n",
    "video_dir = \"./videos/bedroom\"\n",
    "\n",
    "# scan all the JPEG frame names in this directory\n",
    "frame_names = [\n",
    "    p for p in os.listdir(video_dir)\n",
    "    if os.path.splitext(p)[-1] in [\".jpg\", \".jpeg\", \".JPG\", \".JPEG\"]\n",
    "]\n",
    "frame_names.sort(key=lambda p: int(os.path.splitext(p)[0]))\n",
    "\n",
    "# take a look the first video frame\n",
    "frame_idx = 0\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {frame_idx}\")\n",
    "plt.imshow(Image.open(os.path.join(video_dir, frame_names[frame_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff46b10-c17a-4a26-8004-8c6d80806b0a",
   "metadata": {},
   "source": [
    "#### Initialize the inference state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f594ac71-a6b9-461d-af27-500fa1d1a420",
   "metadata": {},
   "source": [
    "SAM 2 requires stateful inference for interactive video segmentation, so we need to initialize an **inference state** on this video.\n",
    "\n",
    "During initialization, it loads all the JPEG frames in `video_path` and stores their pixels in `inference_state` (as shown in the progress bar below).\n",
    "\n",
    "SAM 2 需要有状态推理才能进行交互式视频分段，因此我们需要在此视频上初始化 **推理状态**。\n",
    "\n",
    "在初始化过程中，它会在 'video_path' 中加载所有 JPEG 帧，并将其像素存储在 'inference_state' 中（如下面的进度条所示）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8967aed3-eb82-4866-b8df-0f4743255c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_state = predictor.init_state(video_path=video_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb1f3f6-d74d-4016-934c-8d2a14d1a543",
   "metadata": {},
   "source": [
    "### Example 1: Segment & track one object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2d3127-67b2-45d2-9f32-8fe3e10dc5eb",
   "metadata": {},
   "source": [
    "Note: if you have run any previous tracking using this `inference_state`, please reset it first via `reset_state`.\n",
    "\n",
    "(The cell below is just for illustration; it's not needed to call `reset_state` here as this `inference_state` is just freshly initialized above.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2646a1d-3401-438c-a653-55e0e56b7d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.reset_state(inference_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aeb04d-8cba-4f57-95da-6e5a1796003e",
   "metadata": {},
   "source": [
    " #### Step 1: Add a first click on a frame\n",
    " 在帧上添加第一个点击"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695c7749-b523-4691-aad0-7558c5d1d68c",
   "metadata": {},
   "source": [
    "To get started, let's try to segment the child on the left.\n",
    "\n",
    "Here we make a **positive click** at (x, y) = (210, 350) with label `1`, by sending their coordinates and labels into the `add_new_points_or_box` API.\n",
    "\n",
    "Note: label `1` indicates a *positive click (to add a region)* while label `0` indicates a *negative click (to remove a region)*.\n",
    "\n",
    "首先，让我们尝试对左侧的子对象进行分段。\n",
    "\n",
    "在这里，我们在 （x， y） = （210， 350） 处进行一次 **正向点击**，标签为 '1'，方法是将它们的坐标和标签发送到 'add_new_points_or_box' API。\n",
    "\n",
    "注意：标签“1”表示 *正数点击（添加区域）*，而标签“0”表示 *负数点击（删除区域）*。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e749bab-0f36-4173-bf8d-0c20cd5214b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_frame_idx = 0  # the frame index we interact with\n",
    "ann_obj_id = 1  # give a unique id to each object we interact with (it can be any integers)\n",
    "\n",
    "# Let's add a positive click at (x, y) = (210, 350) to get started\n",
    "points = np.array([[210, 350]], dtype=np.float32)\n",
    "# for labels, `1` means positive click and `0` means negative click\n",
    "labels = np.array([1], np.int32)\n",
    "_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    points=points,\n",
    "    labels=labels,\n",
    ")\n",
    "\n",
    "# show the results on the current (interacted) frame\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx}\")\n",
    "plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n",
    "show_points(points, labels, plt.gca())\n",
    "show_mask((out_mask_logits[0] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89457875-93fa-40ed-b6dc-4e1c971a27f9",
   "metadata": {},
   "source": [
    "#### Step 2: Add a second click to refine the prediction\n",
    "添加第二次点击以优化预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75eb21b-1413-452c-827b-a04093c30c78",
   "metadata": {},
   "source": [
    "Hmm, it seems that although we wanted to segment the child on the left, the model predicts the mask for only the shorts -- this can happen since there is ambiguity from a single click about what the target object should be. We can refine the mask on this frame via another positive click on the child's shirt.\n",
    "\n",
    "Here we make a **second positive click** at (x, y) = (250, 220) with label `1` to expand the mask.\n",
    "\n",
    "Note: we need to send **all the clicks and their labels** (i.e. not just the last click) when calling `add_new_points_or_box`.\n",
    "\n",
    "嗯，虽然我们想分割左侧的孩子，但模型只预测了短裤的掩码 -- 这可能是因为单击一下目标对象应该是什么就存在歧义。我们可以通过再次正向点击孩子的衬衫来优化此框架上的蒙版。\n",
    "\n",
    "在这里，我们在 （x， y） = （250， 220） 处进行 **第二次正向单击** 标签 '1' 以扩展蒙版。\n",
    "\n",
    "注意：在调用 'add_new_points_or_box' 时，我们需要发送 **所有点击及其标签**（即不仅仅是最后一次点击）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ab3ec7-2537-4158-bf98-3d0977d8908d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_frame_idx = 0  # the frame index we interact with\n",
    "ann_obj_id = 1  # give a unique id to each object we interact with (it can be any integers)\n",
    "\n",
    "# Let's add a 2nd positive click at (x, y) = (250, 220) to refine the mask\n",
    "# sending all clicks (and their labels) to `add_new_points_or_box`\n",
    "points = np.array([[210, 350], [250, 220]], dtype=np.float32)\n",
    "# for labels, `1` means positive click and `0` means negative click\n",
    "labels = np.array([1, 1], np.int32)\n",
    "_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    points=points,\n",
    "    labels=labels,\n",
    ")\n",
    "\n",
    "# show the results on the current (interacted) frame\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx}\")\n",
    "plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n",
    "show_points(points, labels, plt.gca())\n",
    "show_mask((out_mask_logits[0] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4ab457-d91d-4ac8-b350-fbcd549fd3fd",
   "metadata": {},
   "source": [
    "With this 2nd refinement click, now we get a segmentation mask of the entire child on frame 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52015ac-1b7b-4c59-bca3-c2b28484cf46",
   "metadata": {},
   "source": [
    "#### Step 3: Propagate the prompts to get the masklet across the video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b025bd-cd58-4bfb-9572-c8d2fd0a02ef",
   "metadata": {},
   "source": [
    "To get the masklet throughout the entire video, we propagate the prompts using the `propagate_in_video` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab45e932-b0d5-4983-9718-6ee77d1ac31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run propagation throughout the video and collect the results in a dict\n",
    "video_segments = {}  # video_segments contains the per-frame segmentation results\n",
    "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
    "    video_segments[out_frame_idx] = {\n",
    "        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "        for i, out_obj_id in enumerate(out_obj_ids)\n",
    "    }\n",
    "\n",
    "# render the segmentation results every few frames\n",
    "vis_frame_stride = 30\n",
    "plt.close(\"all\")\n",
    "for out_frame_idx in range(0, len(frame_names), vis_frame_stride):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.title(f\"frame {out_frame_idx}\")\n",
    "    plt.imshow(Image.open(os.path.join(video_dir, frame_names[out_frame_idx])))\n",
    "    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n",
    "        show_mask(out_mask, plt.gca(), obj_id=out_obj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e801b70-72df-4a72-b3fe-84f145e5e3f6",
   "metadata": {},
   "source": [
    "#### Step 4: Add new prompts to further refine the masklet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478958ab-29b4-4a75-bba4-adb1b03d0a2b",
   "metadata": {},
   "source": [
    "It appears that in the output masklet above, there are some small imperfections in boundary details on frame 150.\n",
    "\n",
    "With SAM 2 we can fix the model predictions interactively. We can add a **negative click** at (x, y) = (82, 415) on this frame with label `0` to refine the masklet. Here we call the `add_new_points_or_box` API with a different `frame_idx` argument to indicate the frame index we want to refine.\n",
    "\n",
    "在上面的输出掩码中，第 150 帧的边界细节似乎存在一些小的缺陷。\n",
    "\n",
    "借助 SAM 2，我们可以交互式地修复模型预测。我们可以在标签为 '0' 的帧的 （x， y） = （82， 415） 处添加一个 **负单击** 来优化蒙版。在这里，我们使用不同的 'frame_idx' 参数调用 'add_new_points_or_box' API，以指示我们要优化的帧索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a572ea9-5b7e-479c-b30c-93c38b121131",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_frame_idx = 150  # further refine some details on this frame\n",
    "ann_obj_id = 1  # give a unique id to the object we interact with (it can be any integers)\n",
    "\n",
    "# show the segment before further refinement\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx} -- before refinement\")\n",
    "plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n",
    "show_mask(video_segments[ann_frame_idx][ann_obj_id], plt.gca(), obj_id=ann_obj_id)\n",
    "\n",
    "# Let's add a negative click on this frame at (x, y) = (82, 415) to refine the segment\n",
    "points = np.array([[82, 410]], dtype=np.float32)\n",
    "# for labels, `1` means positive click and `0` means negative click\n",
    "labels = np.array([0], np.int32)\n",
    "_, _, out_mask_logits = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    points=points,\n",
    "    labels=labels,\n",
    ")\n",
    "\n",
    "# show the segment after the further refinement\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx} -- after refinement\")\n",
    "plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n",
    "show_points(points, labels, plt.gca())\n",
    "show_mask((out_mask_logits > 0.0).cpu().numpy(), plt.gca(), obj_id=ann_obj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a3950a-acf1-435c-bd64-94297267b5e9",
   "metadata": {},
   "source": [
    "#### Step 5: Propagate the prompts (again) to get the masklet across the video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1954ecf-c2ec-4f9c-8d10-c4f527a10cd2",
   "metadata": {},
   "source": [
    "Let's get an updated masklet for the entire video. Here we call `propagate_in_video` again to propagate all the prompts after adding the new refinement click above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa96690-4a38-4a24-aa17-fd2f4db0e232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run propagation throughout the video and collect the results in a dict\n",
    "video_segments = {}  # video_segments contains the per-frame segmentation results\n",
    "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
    "    video_segments[out_frame_idx] = {\n",
    "        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "        for i, out_obj_id in enumerate(out_obj_ids)\n",
    "    }\n",
    "\n",
    "# render the segmentation results every few frames\n",
    "vis_frame_stride = 30\n",
    "plt.close(\"all\")\n",
    "for out_frame_idx in range(0, len(frame_names), vis_frame_stride):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.title(f\"frame {out_frame_idx}\")\n",
    "    plt.imshow(Image.open(os.path.join(video_dir, frame_names[out_frame_idx])))\n",
    "    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n",
    "        show_mask(out_mask, plt.gca(), obj_id=out_obj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607507e3-6a2b-4fd7-944c-2371bdab9d01",
   "metadata": {},
   "source": [
    "The segments now look good on all frames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2502bb5a-3e1f-43d0-9f58-33f8676fff0d",
   "metadata": {},
   "source": [
    "### Example 2: Segment an object using box prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2d26c8-0432-48c6-997e-4a3b77bb5f6d",
   "metadata": {},
   "source": [
    "Note: if you have run any previous tracking using this `inference_state`, please reset it first via `reset_state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbe9183-abbb-4283-b0cb-d24f3d7beb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.reset_state(inference_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb6eae9-0f4c-434f-8089-a46c9ca59da5",
   "metadata": {},
   "source": [
    "In addition to using clicks as inputs, SAM 2 also supports segmenting and tracking objects in a video via **bounding boxes**.\n",
    "\n",
    "In the example below, we segment the child on the right using a **box prompt** of (x_min, y_min, x_max, y_max) = (300, 0, 500, 400) on frame 0 as input into the `add_new_points_or_box` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbfb273-4e14-495b-bd89-87a8baf52ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_frame_idx = 0  # the frame index we interact with\n",
    "ann_obj_id = 4  # give a unique id to each object we interact with (it can be any integers)\n",
    "\n",
    "# Let's add a box at (x_min, y_min, x_max, y_max) = (300, 0, 500, 400) to get started\n",
    "box = np.array([300, 0, 500, 400], dtype=np.float32)\n",
    "_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    box=box,\n",
    ")\n",
    "\n",
    "# show the results on the current (interacted) frame\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx}\")\n",
    "plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n",
    "show_box(box, plt.gca())\n",
    "show_mask((out_mask_logits[0] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3f9ba7-bf4d-47e5-9b02-8a424cab42cc",
   "metadata": {},
   "source": [
    "Here, SAM 2 gets a pretty good segmentation mask of the entire child, even though the input bounding box is not perfectly tight around the object.\n",
    "\n",
    "Similar to the previous example, if the returned mask from is not perfect when using a box prompt, we can also further **refine** the output using positive or negative clicks. To illustrate this, here we make a **positive click** at (x, y) = (460, 60) with label `1` to expand the segment around the child's hair.\n",
    "\n",
    "Note: to refine the segmentation mask from a box prompt, we need to send **both the original box input and all subsequent refinement clicks and their labels** when calling `add_new_points_or_box`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54906315-ab4c-4088-b866-4c22134d5b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_frame_idx = 0  # the frame index we interact with\n",
    "ann_obj_id = 4  # give a unique id to each object we interact with (it can be any integers)\n",
    "\n",
    "# Let's add a positive click at (x, y) = (460, 60) to refine the mask\n",
    "points = np.array([[460, 60]], dtype=np.float32)\n",
    "# for labels, `1` means positive click and `0` means negative click\n",
    "labels = np.array([1], np.int32)\n",
    "# note that we also need to send the original box input along with\n",
    "# the new refinement click together into `add_new_points_or_box`\n",
    "box = np.array([300, 0, 500, 400], dtype=np.float32)\n",
    "_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    points=points,\n",
    "    labels=labels,\n",
    "    box=box,\n",
    ")\n",
    "\n",
    "# show the results on the current (interacted) frame\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx}\")\n",
    "plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n",
    "show_box(box, plt.gca())\n",
    "show_points(points, labels, plt.gca())\n",
    "show_mask((out_mask_logits[0] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73128cd6-dbfa-49f7-8d79-1a8e19835f7f",
   "metadata": {},
   "source": [
    "Then, to get the masklet throughout the entire video, we propagate the prompts using the `propagate_in_video` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd90557-a0dc-442e-b091-9c74c831bef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run propagation throughout the video and collect the results in a dict\n",
    "video_segments = {}  # video_segments contains the per-frame segmentation results\n",
    "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
    "    video_segments[out_frame_idx] = {\n",
    "        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "        for i, out_obj_id in enumerate(out_obj_ids)\n",
    "    }\n",
    "\n",
    "# render the segmentation results every few frames\n",
    "vis_frame_stride = 30\n",
    "plt.close(\"all\")\n",
    "for out_frame_idx in range(0, len(frame_names), vis_frame_stride):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.title(f\"frame {out_frame_idx}\")\n",
    "    plt.imshow(Image.open(os.path.join(video_dir, frame_names[out_frame_idx])))\n",
    "    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n",
    "        show_mask(out_mask, plt.gca(), obj_id=out_obj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e023f91f-0cc5-4980-ae8e-a13c5749112b",
   "metadata": {},
   "source": [
    "Note that in addition to clicks or boxes, SAM 2 also supports directly using a **mask prompt** as input via the `add_new_mask` method in the `SAM2VideoPredictor` class. This can be helpful in e.g. semi-supervised VOS evaluations (see [tools/vos_inference.py](https://github.com/facebookresearch/sam2/blob/main/tools/vos_inference.py) for an example)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da018be8-a4ae-4943-b1ff-702c2b89cb68",
   "metadata": {},
   "source": [
    "### Example 3: Segment multiple objects simultaneously"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea6c04c-3072-4876-b394-879321a48c4a",
   "metadata": {},
   "source": [
    "Note: if you have run any previous tracking using this `inference_state`, please reset it first via `reset_state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b874c8-9f39-42d3-a667-54a0bd696410",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.reset_state(inference_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f3f7e6-4821-468c-84e4-f3a0435c9149",
   "metadata": {},
   "source": [
    "#### Step 1: Add two objects on a frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95158714-86d7-48a9-8365-b213f97cc9ca",
   "metadata": {},
   "source": [
    "SAM 2 can also segment and track two or more objects at the same time. One way, of course, is to do them one by one. However, it would be more efficient to batch them together (e.g. so that we can share the image features between objects to reduce computation costs).\n",
    "\n",
    "This time, let's focus on object parts and segment **the shirts of both childen** in this video. Here we add prompts for these two objects and assign each of them a unique object id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22d896d-3cd5-4fa0-9230-f33e217035dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = {}  # hold all the clicks we add for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d9ac57-b14a-4237-828d-927e422c518b",
   "metadata": {},
   "source": [
    "Add the first object (the left child's shirt) with a **positive click** at (x, y) = (200, 300) on frame 0.\n",
    "\n",
    "We assign it to object id `2` (it can be arbitrary integers, and only needs to be unique for each object to track), which is passed to the `add_new_points_or_box` API to distinguish the object we are clicking upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13432fc-f467-44d8-adfe-3e0c488046b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_frame_idx = 0  # the frame index we interact with\n",
    "ann_obj_id = 2  # give a unique id to each object we interact with (it can be any integers)\n",
    "\n",
    "# Let's add a positive click at (x, y) = (200, 300) to get started on the first object\n",
    "points = np.array([[200, 300]], dtype=np.float32)\n",
    "# for labels, `1` means positive click and `0` means negative click\n",
    "labels = np.array([1], np.int32)\n",
    "prompts[ann_obj_id] = points, labels\n",
    "_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    points=points,\n",
    "    labels=labels,\n",
    ")\n",
    "\n",
    "# show the results on the current (interacted) frame\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx}\")\n",
    "plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n",
    "show_points(points, labels, plt.gca())\n",
    "for i, out_obj_id in enumerate(out_obj_ids):\n",
    "    show_points(*prompts[out_obj_id], plt.gca())\n",
    "    show_mask((out_mask_logits[i] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbbd51b-e1e2-4c36-99ec-1d9a1b49b0cd",
   "metadata": {},
   "source": [
    "Hmm, this time we just want to select the child's shirt, but the model predicts the mask for the entire child. Let's refine the prediction with a **negative click** at (x, y) = (275, 175)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ecf61d-662b-4f98-ae62-46557b219842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the first object\n",
    "ann_frame_idx = 0  # the frame index we interact with\n",
    "ann_obj_id = 2  # give a unique id to each object we interact with (it can be any integers)\n",
    "\n",
    "# Let's add a 2nd negative click at (x, y) = (275, 175) to refine the first object\n",
    "# sending all clicks (and their labels) to `add_new_points_or_box`\n",
    "points = np.array([[200, 300], [275, 175]], dtype=np.float32)\n",
    "# for labels, `1` means positive click and `0` means negative click\n",
    "labels = np.array([1, 0], np.int32)\n",
    "prompts[ann_obj_id] = points, labels\n",
    "_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    points=points,\n",
    "    labels=labels,\n",
    ")\n",
    "\n",
    "# show the results on the current (interacted) frame\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx}\")\n",
    "plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n",
    "show_points(points, labels, plt.gca())\n",
    "for i, out_obj_id in enumerate(out_obj_ids):\n",
    "    show_points(*prompts[out_obj_id], plt.gca())\n",
    "    show_mask((out_mask_logits[i] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194718c1-734d-446c-a3ef-361057de2f31",
   "metadata": {},
   "source": [
    "After the 2nd negative click, now we get the left child's shirt as our first object.\n",
    "\n",
    "Let's move on to the second object (the right child's shirt) with a positive click at (x, y) = (400, 150) on frame 0. Here we assign object id `3` to this second object (it can be arbitrary integers, and only needs to be unique for each object to track).\n",
    "\n",
    "Note: when there are multiple objects, the `add_new_points_or_box` API will return a list of masks for each object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ca1bde-62a4-40e6-98e4-15606441e52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_frame_idx = 0  # the frame index we interact with\n",
    "ann_obj_id = 3  # give a unique id to each object we interact with (it can be any integers)\n",
    "\n",
    "# Let's now move on to the second object we want to track (giving it object id `3`)\n",
    "# with a positive click at (x, y) = (400, 150)\n",
    "points = np.array([[400, 150]], dtype=np.float32)\n",
    "# for labels, `1` means positive click and `0` means negative click\n",
    "labels = np.array([1], np.int32)\n",
    "prompts[ann_obj_id] = points, labels\n",
    "\n",
    "# `add_new_points_or_box` returns masks for all objects added so far on this interacted frame\n",
    "_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    points=points,\n",
    "    labels=labels,\n",
    ")\n",
    "\n",
    "# show the results on the current (interacted) frame on all objects\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx}\")\n",
    "plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n",
    "show_points(points, labels, plt.gca())\n",
    "for i, out_obj_id in enumerate(out_obj_ids):\n",
    "    show_points(*prompts[out_obj_id], plt.gca())\n",
    "    show_mask((out_mask_logits[i] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f7add8-d577-4597-ae2f-654b8c7b05e0",
   "metadata": {},
   "source": [
    "This time the model predicts the mask of the shirt we want to track in just one click. Nice!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448733b8-ea8b-4078-995f-b676c3b558ba",
   "metadata": {},
   "source": [
    "#### Step 2: Propagate the prompts to get masklets across the video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bd73de-d669-41c8-b6ba-943883f0caa2",
   "metadata": {},
   "source": [
    "Now, we propagate the prompts for both objects to get their masklets throughout the video.\n",
    "\n",
    "Note: when there are multiple objects, the `propagate_in_video` API will return a list of masks for each object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17737191-d62b-4611-b2c6-6d0418a9ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run propagation throughout the video and collect the results in a dict\n",
    "video_segments = {}  # video_segments contains the per-frame segmentation results\n",
    "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
    "    video_segments[out_frame_idx] = {\n",
    "        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "        for i, out_obj_id in enumerate(out_obj_ids)\n",
    "    }\n",
    "\n",
    "# render the segmentation results every few frames\n",
    "vis_frame_stride = 30\n",
    "plt.close(\"all\")\n",
    "for out_frame_idx in range(0, len(frame_names), vis_frame_stride):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.title(f\"frame {out_frame_idx}\")\n",
    "    plt.imshow(Image.open(os.path.join(video_dir, frame_names[out_frame_idx])))\n",
    "    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n",
    "        show_mask(out_mask, plt.gca(), obj_id=out_obj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a0b9d7-c78f-432b-afb0-11f2ea5b652a",
   "metadata": {},
   "source": [
    "Looks like both children's shirts are well segmented in this video.\n",
    "\n",
    "Now you can try SAM 2 on your own videos and use cases! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
